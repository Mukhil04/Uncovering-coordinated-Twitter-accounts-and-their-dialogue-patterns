{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# journalist_names = [\"Linling_Wei\", \"Muyi_Xiao\", \"Alice_Su\", \"Marianna Spring\", \"Jiayang Fan\"]\n",
    "# journalist_user_ids = [\"499603787\", \"1616608316\", \"24709718\", \"613832082\", \"418780384\"]\n",
    "# file_names = [\"Lingling_Wei_context_5000.csv\", \"muyixiao_494.csv\", \"aliceysu_convs_all.csv\", \"mariannaspring_convs.csv\", \"JiayangFan_convs.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "journalist_names = [\"Alice_Su\", \"Lingling_Wei\", \"Marianna_Spring\", \"Mei Fong\", \"Muyi_Xiao\", \"Sagarika Ghose\", \"Bainjal\", \"Carole Cadwalldr\", \"Ghada Oueiss\", \"Jiayang Fan\", \"Nighat Dad\", \"Rana Ayyub\", \"Sally Kohn\"]\n",
    "journalist_user_ids = [\"24709718\", \"499603787\", \"613832082\", \"14742251\", \"1616608316\", \"56312411\", \"89732309\", \"722242009\", \"2782521229\", \"418780384\", \"111011528\", \"268676434\", \"18978610\"]\n",
    "file_names = [\"aliceysu_convs.csv\", \"Lingling_wei_convs.csv\", \"mariannaspring_convs.csv\", \"meifongwriter_conv_labels.csv\", \"muyixiao_conv_labels.csv\", \"sagarikaghose_convs.csv\", \"bainjal_conv_labeled.csv\",\"carolecadwalla_conv_labeled.csv\", \"ghadaoueiss_conv_labeled.csv\", \"JiayangFan_conv_labels.csv\", \"nighatdad_conv_labeled.csv\", \"RanaAyuub_conv_labeled.csv\", \"sallykohn_conv_labeled.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# journalist_names = []\n",
    "# journalist_user_ids = []\n",
    "# file_names = []\n",
    "# for i in range(len(journalist_names_raw)):\n",
    "#     journalist_names.append([journalist_names_raw[i]])\n",
    "#     journalist_user_ids.append([journalist_user_ids_raw[i]])\n",
    "#     file_names.append([file_names_raw[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "import pytensor.tensor as pt\n",
    "from pytensor import shared\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from pyvis.network import Network\n",
    "import pyvis\n",
    "from scipy.special import softmax as s\n",
    "from scipy.stats import chisquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation_Tree():\n",
    "    def __init__(self, root, edge_list, tweet_type_list):\n",
    "        self.root = root\n",
    "        self.edge_list = edge_list\n",
    "        self.tweet_type_list = tweet_type_list\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, data=None):\n",
    "        self.data = data\n",
    "        self.children = []\n",
    "\n",
    "class Output_Row():\n",
    "    def __init__(self, tweet_id, tweet_type, parent_tweet_id, parent_tweet_type, child_tweet_ids, child_tweet_type, hop_length):\n",
    "        self.tweet_id = tweet_id\n",
    "        self.tweet_type = tweet_type\n",
    "        self.parent_tweet_id = parent_tweet_id\n",
    "        self.parent_tweet_type = parent_tweet_type\n",
    "        self.child_tweet_ids = child_tweet_ids\n",
    "        self.child_tweet_type = child_tweet_type\n",
    "        self.hop_to_root = hop_length\n",
    "\n",
    "def create_tree(edges):\n",
    "    node_keys = set(key for keys in edges for key in keys)\n",
    "    nodes = { key: Node(key) for key in node_keys }\n",
    "\n",
    "    for parent, child in edges:\n",
    "        nodes[parent].children.append(nodes[child])\n",
    "        node_keys.remove(child)\n",
    "\n",
    "    dangling_and_root_tweets = []\n",
    "    for root_key in node_keys:\n",
    "        dangling_and_root_tweets.append(nodes[root_key])\n",
    "    return dangling_and_root_tweets, nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pymc_Model:\n",
    "    def __init__(self, file_name, journalist_name, journalist_user_id, journalist_index):\n",
    "        self.file_name = file_name\n",
    "        self.journalist_name = journalist_name\n",
    "        self.journalist_user_id = journalist_user_id\n",
    "\n",
    "        self.conversation = []\n",
    "\n",
    "        self.attacker_tweets = []\n",
    "        self.bystander_tweets = []\n",
    "        self.supporter_tweets = []\n",
    "\n",
    "        self.attacker_users = []\n",
    "        self.bystander_users = []\n",
    "        self.supporter_users = []\n",
    "\n",
    "        self.outputs = []\n",
    "        self.prior_frequencies_sorted = []\n",
    "\n",
    "        self.total_attacker_tweets = 0\n",
    "        self.total_bystander_tweets = 0\n",
    "        self.total_supporter_tweets = 0\n",
    "        self.total_journalist_tweets = 0\n",
    "\n",
    "        self.child_type_outcomes = []\n",
    "\n",
    "        self.journalist_index = journalist_index\n",
    "    \n",
    "    def obtain_conversation_df(self):\n",
    "        df = pd.read_csv(self.file_name)\n",
    "        aggregated_labels = df.groupby('user_id')['labels'].agg(list).reset_index()\n",
    "        df1 = aggregated_labels[aggregated_labels['labels'].apply(lambda x: 0 in x)]\n",
    "        df2 = aggregated_labels[(~aggregated_labels['labels'].apply(lambda x: 0 in x)) & (aggregated_labels['labels'].apply(lambda x: 2 in x))]\n",
    "        df3 = aggregated_labels[(~aggregated_labels['labels'].apply(lambda x: 0 in x)) & (~aggregated_labels['labels'].apply(lambda x: 2 in x))]\n",
    "\n",
    "        df['Group'] = df['user_id'].apply(lambda x: 0 if x in df1['user_id'].values else (2 if x in df2['user_id'].values else 1))\n",
    "        self.df_clean = df\n",
    "        print(len(df))\n",
    "        print(f\"Number of rows in df_clean for journalist {self.journalist_name}: {len(df)}\")\n",
    "        conversation_ids = set(df['conversation_id'].unique())\n",
    "        print(f\"Number of unique conversations in this set of data: {len(conversation_ids)}\")\n",
    "        print(f\"Number of tweets authored by journalist {self.journalist_name}:{len(df[df['user_id'] == int(self.journalist_user_id)])}\")\n",
    "        tweets_by_conversation = df.groupby('conversation_id').agg(list).to_dict()\n",
    "        self.conversation_data = {}\n",
    "        for i, key1 in enumerate(tweets_by_conversation.keys()):\n",
    "            for j, key2 in enumerate(tweets_by_conversation[key1].keys()):\n",
    "                if key2 not in self.conversation_data:\n",
    "                    self.conversation_data[key2] = {}\n",
    "                    self.conversation_data[key2][key1] = tweets_by_conversation[key1][key2]\n",
    "                else:\n",
    "                    self.conversation_data[key2][key1] = tweets_by_conversation[key1][key2]\n",
    "\n",
    "    def obtain_specific_conversation(self):\n",
    "        for key in self.conversation_data.keys():\n",
    "            root_temp = key\n",
    "            edges_temp = []\n",
    "            tweet_type_temp = []\n",
    "            for j in range(len(self.conversation_data[key]['tweet_id'])):\n",
    "                if self.conversation_data[key]['user_id'][j] == int(self.journalist_user_id):\n",
    "                    tweet_type_temp.append(3)\n",
    "                else:\n",
    "                    tweet_type_temp.append(self.conversation_data[key]['Group'][j])\n",
    "                edges_temp.append((self.conversation_data[key]['reference_id'][j], self.conversation_data[key]['tweet_id'][j]))\n",
    "                \n",
    "            \n",
    "            self.conversation.append(Conversation_Tree(root_temp, edges_temp, tweet_type_temp))\n",
    "        \n",
    "    def setting_up_conversation_trees(self):\n",
    "        self.conversation_tree_roots = []\n",
    "        self.tree_node_map = {}\n",
    "        for j in range(len(self.conversation)):\n",
    "            conversation = self.conversation[j]  \n",
    "            if len(conversation.edge_list) == 0:\n",
    "                self.conversation_tree_roots.append(Node(conversation.root))\n",
    "                dict_temp = {conversation.root : Node(conversation.root)}\n",
    "                self.tree_node_map.update(dict_temp)\n",
    "            else:\n",
    "                root_node, node_map = create_tree(conversation.edge_list) \n",
    "                self.tree_node_map.update(node_map)\n",
    "                for node in root_node:\n",
    "                    if node.data == conversation.root and len(conversation.edge_list) > 0:\n",
    "                        self.conversation_tree_roots.append(node)\n",
    "        \n",
    "\n",
    "    def obtain_sets_of_tweet_types(self):\n",
    "        self.attacker_tweets.append(self.df_clean[(self.df_clean['labels'] == 0) & (self.df_clean['user_id'] != int(self.journalist_user_id))]['tweet_id'].to_list())\n",
    "        self.bystander_tweets.append(self.df_clean[(self.df_clean['labels'] == 1) & (self.df_clean['user_id'] != int(self.journalist_user_id))]['tweet_id'].to_list())\n",
    "        self.supporter_tweets.append(self.df_clean[(self.df_clean['labels'] == 2) & (self.df_clean['user_id'] != int(self.journalist_user_id))]['tweet_id'].to_list())\n",
    "        \n",
    "\n",
    "    def obtain_sets_of_user_types(self):\n",
    "        self.attacker_users = self.df_clean[(self.df_clean['Group'] == 0) & (self.df_clean['user_id'] != int(self.journalist_user_id))]['tweet_id'].to_list()\n",
    "        self.bystander_users = self.df_clean[(self.df_clean['Group'] == 1) & (self.df_clean['user_id'] != int(self.journalist_user_id))]['tweet_id'].to_list()\n",
    "        self.supporter_users = self.df_clean[(self.df_clean['Group'] == 2) & (self.df_clean['user_id'] != int(self.journalist_user_id))]['tweet_id'].to_list()\n",
    "        \n",
    "    def recursive_tree_traversal(self, root_node, depth, max_depth, i):\n",
    "        if len(root_node.children) == 0:\n",
    "            root_node.depth = depth\n",
    "            if root_node.data in self.attacker_users:\n",
    "                self.total_attacker_tweets += 1\n",
    "                root_node.type = 0\n",
    "            elif root_node.data in self.bystander_users:\n",
    "                self.total_bystander_tweets += 1\n",
    "                root_node.type = 1\n",
    "            elif root_node.data in self.supporter_users:\n",
    "                self.total_supporter_tweets += 1\n",
    "                root_node.type = 2\n",
    "            else:\n",
    "                root_node.type = 3\n",
    "                self.total_journalist_tweets += 1\n",
    "            return max(depth, max_depth)\n",
    "        else:\n",
    "            max_depth_temp = max_depth\n",
    "            for child in root_node.children:\n",
    "                max_depth_temp = max(max_depth_temp, self.recursive_tree_traversal(child, depth+1, max_depth, i))\n",
    "            root_node.depth = depth\n",
    "            if root_node.data in self.attacker_users:\n",
    "                self.total_attacker_tweets += 1\n",
    "                root_node.type = 0\n",
    "            elif root_node.data in self.bystander_users:\n",
    "                self.total_bystander_tweets += 1\n",
    "                root_node.type = 1\n",
    "            elif root_node.data in self.supporter_users:\n",
    "                self.total_supporter_tweets += 1\n",
    "                root_node.type = 2\n",
    "            else:\n",
    "                root_node.type = 3\n",
    "                self.total_journalist_tweets += 1\n",
    "            return max_depth_temp\n",
    "        \n",
    "    def assigning_depth_and_type_to_nodes(self):\n",
    "        master_depth = 0\n",
    "        for tree_root in self.conversation_tree_roots:\n",
    "            master_depth = max(self.recursive_tree_traversal(tree_root,1, 0, i), master_depth)\n",
    "        self.master_depth = master_depth\n",
    "\n",
    "    def update_tree_node_map(self):\n",
    "        for j in range(len(self.conversation)):\n",
    "            try:    \n",
    "                self.tree_node_map[self.conversation[j].root].parent_tweet_id = None\n",
    "                self.tree_node_map[self.conversation[j].root].parent_tweet_type = None\n",
    "                child_tweet_ids_temp = []\n",
    "                child_tweet_types_temp = []\n",
    "                for kid in self.tree_node_map[self.conversation[j].root].children:\n",
    "                    child_tweet_ids_temp.append(kid.data)\n",
    "                    child_tweet_types_temp.append(kid.type)\n",
    "                if len(child_tweet_ids_temp) == 0:\n",
    "                    self.tree_node_map[self.conversation[j].root].child_tweet_ids = [0]\n",
    "                    self.tree_node_map[self.conversation[j].root].child_tweet_types = [4]\n",
    "                else:\n",
    "                    self.tree_node_map[self.conversation[j].root].child_tweet_ids = child_tweet_ids_temp\n",
    "                    self.tree_node_map[self.conversation[j].root].child_tweet_types = child_tweet_types_temp\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            for k,edge in enumerate(self.conversation[j].edge_list):\n",
    "                parent, child = edge\n",
    "                try:\n",
    "                    self.tree_node_map[child].parent_tweet_type = self.tree_node_map[parent].type\n",
    "                    self.tree_node_map[child].parent_tweet_id = self.tree_node_map[parent].data\n",
    "                except:                     \n",
    "                    pass\n",
    "                child_tweet_ids_temp = []\n",
    "                child_tweet_types_temp = []\n",
    "                try:\n",
    "                    for m, kid in enumerate(self.tree_node_map[child].children):\n",
    "                        child_tweet_types_temp.append(kid.type)\n",
    "                        child_tweet_ids_temp.append(kid.data)\n",
    "                except:                     \n",
    "                    pass\n",
    "                if len(child_tweet_ids_temp) == 0:\n",
    "                    self.tree_node_map[child].child_tweet_ids = [0]\n",
    "                    self.tree_node_map[child].child_tweet_types = [4]\n",
    "                else:\n",
    "                    self.tree_node_map[child].child_tweet_ids = child_tweet_ids_temp\n",
    "                    self.tree_node_map[child].child_tweet_types = child_tweet_types_temp\n",
    "\n",
    "    def converting_tree_node_map(self):\n",
    "        self.outputs = []\n",
    "        error1 = 0\n",
    "        error2 = 0\n",
    "        for key in self.tree_node_map:\n",
    "            try:\n",
    "                dummy = self.tree_node_map[key].type\n",
    "            except:\n",
    "                error1 += 1\n",
    "                continue\n",
    "            try:\n",
    "                dummy = [self.tree_node_map[key].data, self.tree_node_map[key].type, self.tree_node_map[key].parent_tweet_id, self.tree_node_map[key].parent_tweet_type, self.tree_node_map[key].child_tweet_ids, self.tree_node_map[key].child_tweet_types]\n",
    "                try: \n",
    "                    self.outputs.append(Output_Row(self.tree_node_map[key].data, self.tree_node_map[key].type, self.tree_node_map[key].parent_tweet_id, self.tree_node_map[key].parent_tweet_type, self.tree_node_map[key].child_tweet_ids, self.tree_node_map[key].child_tweet_types, self.tree_node_map[key].depth-1))\n",
    "                except:\n",
    "                    self.outputs.append(Output_Row(self.tree_node_map[key].data, self.tree_node_map[key].type, self.tree_node_map[key].parent_tweet_id, self.tree_node_map[key].parent_tweet_type, self.tree_node_map[key].child_tweet_ids, self.tree_node_map[key].child_tweet_types, 0))\n",
    "            except:\n",
    "                error2 += 1   \n",
    "                pass\n",
    "    \n",
    "    def obtain_model_input_output_data(self):\n",
    "        self.child_type_input = {\"parent_tweet_type\" : [], \"tweet_type\" : [], \"hop_to_root\" : []}\n",
    "        self.child_type_output = []\n",
    "        #prior_frequencies = {}\n",
    "        for j, output in enumerate(self.outputs):\n",
    "            if output.parent_tweet_type is None:\n",
    "                continue\n",
    "            for child_type in output.child_tweet_type:\n",
    "                if child_type == 4:\n",
    "                    break\n",
    "                # if (output.parent_tweet_type, output.tweet_type, output.hop_to_root, child_type) in prior_frequencies:\n",
    "                #     prior_frequencies[(output.parent_tweet_type, output.tweet_type, output.hop_to_root, child_type)] += 1\n",
    "                # else:\n",
    "                #     prior_frequencies[(output.parent_tweet_type, output.tweet_type, output.hop_to_root, child_type)] = 1\n",
    "                self.child_type_input[\"parent_tweet_type\"].append(output.parent_tweet_type)\n",
    "                self.child_type_input[\"tweet_type\"].append(output.tweet_type)\n",
    "                self.child_type_input[\"hop_to_root\"].append(output.hop_to_root)\n",
    "                self.child_type_output.append(child_type)\n",
    "            # self.prior_frequencies_sorted.append(sorted(prior_frequencies.items()))\n",
    "\n",
    "    def convert_data_to_csv(self, number):\n",
    "        array1 = self.child_type_input[\"parent_tweet_type\"]\n",
    "        array2 = self.child_type_input[\"tweet_type\"]\n",
    "        array3 = self.child_type_input[\"hop_to_root\"]\n",
    "        array4 = self.child_type_output\n",
    "        array5 = [self.journalist_name] * len(array1)\n",
    "        rows = zip(array1, array2, array3, array4, array5)\n",
    "        if number == 0:\n",
    "            with open('output.csv', 'w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(['Grandparent_tweet_label', 'Parent_tweet_label', 'Depth', 'Child_label(output)'])\n",
    "                writer.writerows(rows)\n",
    "        else:\n",
    "            with open('output.csv', mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerows(rows)\n",
    "\n",
    "\n",
    "    \n",
    "    def bayesian_modelling(self):\n",
    "        N = len(self.child_type_output)\n",
    "        mean = 0\n",
    "        sigma = 1\n",
    "        nu = 2\n",
    "        max_depth = self.master_depth\n",
    "        batch_size = 4000\n",
    "        if len(self.df_clean) < 15000:\n",
    "            with pm.Model() as ChildTypeModelSample:\n",
    "\n",
    "                InterceptAttackerProbability = pm.StudentT('InterceptAttackerProbability',nu=nu, mu = mean, sigma = sigma)\n",
    "                GrandParentTweetTypeAttackerProbability = pm.StudentT('GrandParentTweetTypeAttackerProbability',nu=nu, mu = mean, sigma = sigma, shape=4)\n",
    "                ParentTweetTypeAttackerProbability = pm.StudentT('ParentTweetTypeAttackerProbability',nu=nu, mu = mean, sigma = sigma,shape=4)\n",
    "                HopToRootAttackerProbability = pm.StudentT('HopToRootAttackerProbability',nu=nu, mu = mean, sigma = sigma,shape=max_depth)\n",
    "                \n",
    "                InterceptBystanderProbability = pm.StudentT('InterceptBystanderProbability',nu=nu, mu = mean, sigma = sigma)\n",
    "                GrandParentTweetTypeBystanderProbability = pm.StudentT('GrandParentTweetTypeBystanderProbability',nu=nu, mu = mean, sigma = sigma,shape=4)\n",
    "                ParentTweetTypeBystanderProbability = pm.StudentT('ParentTweetTypeBystanderProbability',nu=nu, mu = mean, sigma = sigma,shape=4)\n",
    "                HopToRootBystanderProbability = pm.StudentT('HopToRootBystanderProbability',nu=nu, mu = mean, sigma = sigma,shape=max_depth) \n",
    "\n",
    "                InterceptSupporterProbability = pm.StudentT('InterceptSupporterProbability',nu=nu, mu = mean, sigma = sigma)\n",
    "                GrandParentTweetTypeSupporterProbability = pm.StudentT('GrandParentTweetTypeSupporterProbability',nu=nu, mu = mean, sigma = sigma,shape=4)\n",
    "                ParentTweetTypeSupporterProbability = pm.StudentT('ParentTweetTypeSupporterProbability',nu=nu, mu = mean, sigma = sigma,shape=4)\n",
    "                HopToRootSupporterProbability = pm.StudentT('HopToRootSupporterProbability',nu=nu, mu = mean, sigma =sigma,shape=max_depth)\n",
    "\n",
    "                s0 = InterceptAttackerProbability + GrandParentTweetTypeAttackerProbability[self.child_type_input['parent_tweet_type']] + ParentTweetTypeAttackerProbability[self.child_type_input['tweet_type']] + HopToRootAttackerProbability[self.child_type_input['hop_to_root']]\n",
    "                s1 = InterceptBystanderProbability + GrandParentTweetTypeBystanderProbability[self.child_type_input['parent_tweet_type']] + ParentTweetTypeBystanderProbability[self.child_type_input['tweet_type']] + HopToRootBystanderProbability[self.child_type_input['hop_to_root']]\n",
    "                s2 = InterceptSupporterProbability + GrandParentTweetTypeSupporterProbability[self.child_type_input['parent_tweet_type']] + ParentTweetTypeSupporterProbability[self.child_type_input['tweet_type']] + HopToRootSupporterProbability[self.child_type_input['hop_to_root']]\n",
    "                s3 = np.zeros(N)\n",
    "                s = pm.math.stack([s0, s1, s2, s3]).T\n",
    "\n",
    "                p_ = pm.math.softmax(s, axis=1)\n",
    "                child_type = pm.Categorical(\"child_type\", p=p_, observed=self.child_type_output, total_size = N)\n",
    "                mean_field_2 = pm.fit(method = 'advi', obj_optimizer = pm.adagrad_window(learning_rate=1e-3))\n",
    "\n",
    "        else:\n",
    "            with pm.Model() as ChildTypeModelSample:\n",
    "                parent_tweet_type, tweet_type, hop_to_root, output = pm.Minibatch(self.child_type_input['parent_tweet_type'], self.child_type_input['tweet_type'], self.child_type_input['hop_to_root'], self.child_type_output, batch_size=batch_size)\n",
    "\n",
    "                InterceptAttackerProbability = pm.StudentT('InterceptAttackerProbability',nu=nu, mu = mean, sigma = sigma)\n",
    "                GrandParentTweetTypeAttackerProbability = pm.StudentT('GrandParentTweetTypeAttackerProbability',nu=nu, mu = mean, sigma = sigma, shape=4)\n",
    "                ParentTweetTypeAttackerProbability = pm.StudentT('ParentTweetTypeAttackerProbability',nu=nu, mu = mean, sigma = sigma,shape=4)\n",
    "                HopToRootAttackerProbability = pm.StudentT('HopToRootAttackerProbability',nu=nu, mu = mean, sigma = sigma,shape=max_depth)\n",
    "                \n",
    "                InterceptBystanderProbability = pm.StudentT('InterceptBystanderProbability',nu=nu, mu = mean, sigma = sigma)\n",
    "                GrandParentTweetTypeBystanderProbability = pm.StudentT('GrandParentTweetTypeBystanderProbability',nu=nu, mu = mean, sigma = sigma,shape=4)\n",
    "                ParentTweetTypeBystanderProbability = pm.StudentT('ParentTweetTypeBystanderProbability',nu=nu, mu = mean, sigma = sigma,shape=4)\n",
    "                HopToRootBystanderProbability = pm.StudentT('HopToRootBystanderProbability',nu=nu, mu = mean, sigma = sigma,shape=max_depth) \n",
    "\n",
    "                InterceptSupporterProbability = pm.StudentT('InterceptSupporterProbability',nu=nu, mu = mean, sigma = sigma)\n",
    "                GrandParentTweetTypeSupporterProbability = pm.StudentT('GrandParentTweetTypeSupporterProbability',nu=nu, mu = mean, sigma = sigma,shape=4)\n",
    "                ParentTweetTypeSupporterProbability = pm.StudentT('ParentTweetTypeSupporterProbability',nu=nu, mu = mean, sigma = sigma,shape=4)\n",
    "                HopToRootSupporterProbability = pm.StudentT('HopToRootSupporterProbability',nu=nu, mu = mean, sigma =sigma,shape=max_depth)\n",
    "\n",
    "                s0 = InterceptAttackerProbability + GrandParentTweetTypeAttackerProbability[parent_tweet_type] + ParentTweetTypeAttackerProbability[tweet_type] + HopToRootAttackerProbability[hop_to_root]\n",
    "                s1 = InterceptBystanderProbability + GrandParentTweetTypeBystanderProbability[parent_tweet_type] + ParentTweetTypeBystanderProbability[tweet_type] + HopToRootBystanderProbability[hop_to_root]\n",
    "                s2 = InterceptSupporterProbability + GrandParentTweetTypeSupporterProbability[parent_tweet_type] + ParentTweetTypeSupporterProbability[tweet_type] + HopToRootSupporterProbability[hop_to_root]\n",
    "                s3 = np.zeros(batch_size)\n",
    "                s = pm.math.stack([s0, s1, s2, s3]).T\n",
    "\n",
    "                p_ = pm.math.softmax(s, axis=1)\n",
    "                child_type = pm.Categorical(\"child_type\", p=p_, observed=output, total_size = batch_size)\n",
    "                mean_field_2 = pm.fit(method = 'advi', obj_optimizer = pm.adagrad_window(learning_rate=1e-3))\n",
    "\n",
    "        with ChildTypeModelSample:\n",
    "            self.child_type_trace = mean_field_2.sample(5000)\n",
    "\n",
    "        with ChildTypeModelSample:\n",
    "            self.posterior_predictive_samples = pm.sample_posterior_predictive(self.child_type_trace, extend_inferencedata=True)\n",
    "        \n",
    "        with ChildTypeModelSample:\n",
    "            pm.compute_log_likelihood(self.child_type_trace)\n",
    "\n",
    "        result = pm.model_to_graphviz(ChildTypeModelSample)\n",
    "        # result.render(filename='model_graph', format='png')\n",
    "\n",
    "    def posterior_ground_truth_distribution(self):\n",
    "        result = self.posterior_predictive_samples.posterior_predictive.to_array()\n",
    "        print(result.shape)\n",
    "        predictions = result[0][0][0]\n",
    "        counter = 0\n",
    "        self.posterior_frequencies = {key: 0 for key in self.prior_frequencies}\n",
    "        for i, output in enumerate(self.outputs):\n",
    "            if output.parent_tweet_type is None:\n",
    "                continue\n",
    "            for child_type in output.child_tweet_type:\n",
    "                if child_type == 4:\n",
    "                    break\n",
    "                if (output.parent_tweet_type, output.tweet_type, output.hop_to_root, int(predictions[counter])) in self.posterior_frequencies:\n",
    "                    self.posterior_frequencies[(output.parent_tweet_type, output.tweet_type, output.hop_to_root, int(predictions[counter]))] += 1\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    self.posterior_frequencies[(output.parent_tweet_type, output.tweet_type, output.hop_to_root, int(predictions[counter]))] = 1\n",
    "                    counter += 1\n",
    "        self.posterior_frequencies_sorted = sorted(self.posterior_frequencies.items())\n",
    "        #posterior_ground_truth_differences = {key: (self.prior_frequencies[key], self.posterior_frequencies.get(key)) for key in self.prior_frequencies if key[2] <=4}\n",
    "        #print(sorted(posterior_ground_truth_differences.items()))\n",
    "\n",
    "    def model_quality(self):\n",
    "        print(az.waic(self.child_type_trace))\n",
    "        print(az.loo(self.child_type_trace))\n",
    "        result = self.posterior_predictive_samples.posterior_predictive.to_array()\n",
    "        predictions = result[0][0][0]\n",
    "        freq_dict_predicted = {0:0, 1:0, 2:0, 3:0}\n",
    "        for i in range(len(predictions)):\n",
    "            if int(predictions[i]) not in freq_dict_predicted:\n",
    "                freq_dict_predicted[int(predictions[i])] = 1\n",
    "            else:\n",
    "                freq_dict_predicted[int(predictions[i])] += 1\n",
    "\n",
    "        # bins = list(freq_dict_predicted.keys())\n",
    "        # counts = list(freq_dict_predicted.values())\n",
    "\n",
    "        df = pd.DataFrame(freq_dict_predicted.items(), columns=[\"Value\", \"Frequency\"])\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(data=df, x=\"Value\", weights=\"Frequency\", bins=len(df), kde=True)\n",
    "        sns.despine()\n",
    "\n",
    "        \n",
    "        plt.xlabel('Child labels')\n",
    "        plt.ylabel('Counts')\n",
    "        # plt.title(f'Histogram of posterior predictive outcome for Journalist {self.journalist_index}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.journalist_index}_posterior.pdf', format='pdf')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "        # plt.bar(bins, counts, color='blue', alpha=0.7)\n",
    "\n",
    "        # plt.title('Histogram of posterior predictive outcome')\n",
    "        # plt.xlabel('Child labels')\n",
    "        # plt.ylabel('Counts')\n",
    "        # sns.despine()\n",
    "        # plt.grid(axis='y')\n",
    "\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "        \n",
    "        freq_dict_expected = {0:0, 1:0, 2:0, 3:0}\n",
    "        expected = random.choices(self.child_type_output, k=len(predictions))\n",
    "        for element in expected:\n",
    "            if element not in freq_dict_expected:\n",
    "                freq_dict_expected[element] = 1\n",
    "            else:\n",
    "                freq_dict_expected[element] += 1\n",
    "        \n",
    "        df = pd.DataFrame(freq_dict_expected.items(), columns=[\"Value\", \"Frequency\"])\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(data=df, x=\"Value\", weights=\"Frequency\", bins=len(df), kde=True)\n",
    "        sns.despine()\n",
    "\n",
    "        plt.xlabel('Child labels')\n",
    "        plt.ylabel('Counts')\n",
    "        # plt.title(f'Histogram of ground truth outcome distribution for Journalist {self.journalist_index}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.journalist_index}_ground_truth.pdf', format='pdf')\n",
    "        plt.show()\n",
    "        \n",
    "        # bins = list(freq_dict_expected.keys())\n",
    "        # counts = list(freq_dict_expected.values())\n",
    "\n",
    "        # plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # plt.bar(bins, counts, color='blue', alpha=0.7)\n",
    "\n",
    "        # plt.title('Histogram of expected data')\n",
    "        # plt.xlabel('Bins')\n",
    "        # plt.ylabel('Counts')\n",
    "        # sns.despine()\n",
    "        # plt.grid(axis='y')\n",
    "\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "\n",
    "        # for element in enumerate(self.prior_frequencies_sorted):\n",
    "        #     if element[1][0] not in freq_dict.keys():\n",
    "        #         freq_dict[element[1][0]] = [element[1][1], 0]\n",
    "        #     else:\n",
    "        #         freq_dict[element[1][0]] = [element[1][1], freq_dict[element[1][0]][1]]\n",
    "        # expected_outcomes = []\n",
    "        # observed_outcomes = []\n",
    "        # for key in freq_dict.keys():\n",
    "        #     expected_outcomes.append(freq_dict[key][0])\n",
    "        #     observed_outcomes.append(freq_dict[key][1])\n",
    "\n",
    "        # expected_outcomes = np.array(expected_outcomes)\n",
    "        # observed_outcomes = np.array(observed_outcomes)\n",
    "        # chi_square_distance = np.sum((expected_outcomes - observed_outcomes)**2 / (expected_outcomes+observed_outcomes))\n",
    "        # print(\"Chi-Square Distance:\", chi_square_distance)      \n",
    "    \n",
    "    def posterior_predictive_outcomes(self):\n",
    "        tweet_class_coding = {0: \"A\", 1: \"B\", 2: \"S\", 3: \"J\"}\n",
    "        child_type_probability = {}\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                for k in range(1,5):\n",
    "                    output_a = self.child_type_trace.posterior.GrandParentTweetTypeAttackerProbability[:,:,i] + self.child_type_trace.posterior.ParentTweetTypeAttackerProbability[:,:,j] + self.child_type_trace.posterior.InterceptAttackerProbability + self.child_type_trace.posterior.HopToRootAttackerProbability[:,:,k]\n",
    "                    output_b = self.child_type_trace.posterior.GrandParentTweetTypeBystanderProbability[:,:,i] + self.child_type_trace.posterior.ParentTweetTypeBystanderProbability[:,:,j] + self.child_type_trace.posterior.InterceptBystanderProbability + self.child_type_trace.posterior.HopToRootBystanderProbability[:,:,k]\n",
    "                    output_s = self.child_type_trace.posterior.GrandParentTweetTypeSupporterProbability[:,:,i] + self.child_type_trace.posterior.ParentTweetTypeSupporterProbability[:,:,j] + self.child_type_trace.posterior.InterceptSupporterProbability + self.child_type_trace.posterior.HopToRootSupporterProbability[:,:,k]\n",
    "\n",
    "                    pre_probability_arr = np.array([output_a, output_b, output_s, np.zeros_like(output_a)])\n",
    "                    pre_probability_arr = np.squeeze(pre_probability_arr).T\n",
    "\n",
    "                    child_type_probability[f\"{str(tweet_class_coding[i])}_{str(tweet_class_coding[j])}_{str(k)}\"] = s(pre_probability_arr, axis = 1)\n",
    "        \n",
    "        self.child_type_outcomes = {}\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                for k in range(1,5):\n",
    "                    if k == 1 and i != 3:\n",
    "                        continue\n",
    "                    self.child_type_outcomes[f\"{str(tweet_class_coding[i])}_{str(tweet_class_coding[j])}_{str(k)}\"] = np.zeros(5000)\n",
    "                    for l in range(child_type_probability[f\"{str(tweet_class_coding[i])}_{str(tweet_class_coding[j])}_{str(k)}\"].shape[0]):\n",
    "                        # print(child_type_probability[f\"{str(tweet_class_coding[i])}_{str(tweet_class_coding[j])}_{str(k)}\"][l].shape, child_type_probability[f\"{str(tweet_class_coding[i])}_{str(tweet_class_coding[j])}_{str(k)}\"][l])\n",
    "                        self.child_type_outcomes[f\"{str(tweet_class_coding[i])}_{str(tweet_class_coding[j])}_{str(k)}\"][l] = random.choices([0,1,2,3], weights = child_type_probability[f\"{str(tweet_class_coding[i])}_{str(tweet_class_coding[j])}_{str(k)}\"][l], k = 1)[0]\n",
    "\n",
    "\n",
    "    def stacked_bar_plot(self, char, depth = 0):\n",
    "        label_map = {0 : \"Attacker (A)\", 1 : \"Bystander (B)\", 2 : \"Supporter (S)\", 3 : \"Journalist (J)\"}\n",
    "        if depth == 1:\n",
    "            df_outcomes = pd.DataFrame(self.child_type_outcomes)\n",
    "            df_filtered = df_outcomes.filter(regex=r'.*1$')\n",
    "        else:\n",
    "            df_outcomes = pd.DataFrame(self.child_type_outcomes)\n",
    "            df_filtered = df_outcomes.filter(regex=fr'^[{char}].*[^1]$')\n",
    "        value_counts = df_filtered.apply(lambda col: col.value_counts(normalize=True).reindex([0, 1, 2, 3], fill_value=0))\n",
    "        fig, ax = plt.subplots(figsize=(12, 3))\n",
    "        palette = sns.color_palette(\"pastel\", 4)\n",
    "        colors = [palette[3], palette[2], palette[0], palette[1]] \n",
    "        bottom = np.zeros(len(df_filtered.columns))\n",
    "        new_axis_labels = []\n",
    "        for item in df_filtered.columns:\n",
    "            latex_string = f\"${item[0]}_{str(int(item[-1])-1)} \\\\rightarrow {item[2]}_{item[-1]}$\"\n",
    "            new_axis_labels.append(latex_string)\n",
    "        for value in [0, 1, 2, 3]:\n",
    "            ax.bar(new_axis_labels, value_counts.loc[value], bottom=bottom, color=colors[value], label=f'{label_map[value]}')\n",
    "            bottom += value_counts.loc[value]\n",
    "\n",
    "        ax.set_xlabel(\"Path History ($GrandparentLabel_{GrandparentDepth} \\\\rightarrow ParentLabel_{ParentDepth}$)\")\n",
    "        ax.set_ylabel('Composition of Child label')\n",
    "        ax.set_title('Distribution of Child label given Grandparent label, Parent label, and Depth')\n",
    "\n",
    "        \n",
    "        ax.legend(title='', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax = plt.gca()  # Get the current axis\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        # Set background color to light grey\n",
    "        ax.set_facecolor('#F0F0F0')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.savefig(f'{self.journalist_name}_{char}_{depth}.pdf', format='pdf')\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "        return value_counts\n",
    "\n",
    "    def plot_stacked_bar_plot(self):\n",
    "        value_counts_A = self.stacked_bar_plot('A')\n",
    "        value_counts_B = self.stacked_bar_plot('B')\n",
    "        value_counts_S = self.stacked_bar_plot('S')\n",
    "        value_counts_J = self.stacked_bar_plot('J')\n",
    "        value_counts_J_1 = self.stacked_bar_plot('J', 1)\n",
    "\n",
    "        return [value_counts_A, value_counts_B, value_counts_S, value_counts_J, value_counts_J_1]\n",
    "        \n",
    "\n",
    "                \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10632\n",
      "Number of rows in df_clean for journalist Alice_Su: 10632\n",
      "Number of unique conversations in this set of data: 419\n",
      "Number of tweets authored by journalist Alice_Su:394\n",
      "4286\n",
      "Number of rows in df_clean for journalist Lingling_Wei: 4286\n",
      "Number of unique conversations in this set of data: 452\n",
      "Number of tweets authored by journalist Lingling_Wei:378\n",
      "40003\n",
      "Number of rows in df_clean for journalist Marianna_Spring: 40003\n",
      "Number of unique conversations in this set of data: 689\n",
      "Number of tweets authored by journalist Marianna_Spring:3334\n",
      "1908\n",
      "Number of rows in df_clean for journalist Mei Fong: 1908\n",
      "Number of unique conversations in this set of data: 443\n",
      "Number of tweets authored by journalist Mei Fong:206\n",
      "378\n",
      "Number of rows in df_clean for journalist Muyi_Xiao: 378\n",
      "Number of unique conversations in this set of data: 16\n",
      "Number of tweets authored by journalist Muyi_Xiao:33\n",
      "10463\n",
      "Number of rows in df_clean for journalist Sagarika Ghose: 10463\n",
      "Number of unique conversations in this set of data: 166\n",
      "Number of tweets authored by journalist Sagarika Ghose:2\n",
      "30000\n",
      "Number of rows in df_clean for journalist Bainjal: 30000\n",
      "Number of unique conversations in this set of data: 539\n",
      "Number of tweets authored by journalist Bainjal:136\n",
      "30000\n",
      "Number of rows in df_clean for journalist Carole Cadwalldr: 30000\n",
      "Number of unique conversations in this set of data: 119\n",
      "Number of tweets authored by journalist Carole Cadwalldr:138\n",
      "30000\n",
      "Number of rows in df_clean for journalist Ghada Oueiss: 30000\n",
      "Number of unique conversations in this set of data: 240\n",
      "Number of tweets authored by journalist Ghada Oueiss:97\n",
      "5078\n",
      "Number of rows in df_clean for journalist Jiayang Fan: 5078\n",
      "Number of unique conversations in this set of data: 392\n",
      "Number of tweets authored by journalist Jiayang Fan:143\n",
      "37964\n",
      "Number of rows in df_clean for journalist Nighat Dad: 37964\n",
      "Number of unique conversations in this set of data: 1919\n",
      "Number of tweets authored by journalist Nighat Dad:1780\n",
      "30000\n",
      "Number of rows in df_clean for journalist Rana Ayyub: 30000\n",
      "Number of unique conversations in this set of data: 77\n",
      "Number of tweets authored by journalist Rana Ayyub:24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mw/3kzv0jk93qd3lfbt8gh59vnh0000gn/T/ipykernel_72975/88783375.py:30: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(self.file_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55283\n",
      "Number of rows in df_clean for journalist Sally Kohn: 55283\n",
      "Number of unique conversations in this set of data: 1884\n",
      "Number of tweets authored by journalist Sally Kohn:0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# obj.bayesian_modelling()\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# # obj.posterior_ground_truth_distribution()\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# obj.model_quality()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# for j in range(len(journalist_value_counts[0])):\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#     journalists_same_path_history.append(journalist_value_counts[0][j], journalist_value_counts[1][j], journalist_value_counts[2][j], journalist_value_counts[3][j])\u001b[39;00m\n\u001b[1;32m     35\u001b[0m all_journalists_same_path_history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mjournalist_value_counts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)):\n\u001b[1;32m     37\u001b[0m     temp_arr \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(journalist_value_counts)):\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "class_objects = []\n",
    "journalist_value_counts = []\n",
    "plt.rcParams['font.size'] = 12        \n",
    "plt.rcParams['axes.titlesize'] = 16    \n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "journalist_index = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M']\n",
    "\n",
    "for i in range(len(journalist_names)):\n",
    "    obj = Pymc_Model(file_names[i], journalist_names[i], journalist_user_ids[i], journalist_index[i])\n",
    "    obj.obtain_conversation_df()\n",
    "    obj.obtain_specific_conversation()\n",
    "    obj.setting_up_conversation_trees()\n",
    "    obj.obtain_sets_of_user_types()\n",
    "    obj.assigning_depth_and_type_to_nodes()\n",
    "    obj.update_tree_node_map()\n",
    "    obj.converting_tree_node_map()\n",
    "    obj.obtain_model_input_output_data()\n",
    "    if i == 0:\n",
    "        obj.convert_data_to_csv(0)\n",
    "    else:\n",
    "        obj.convert_data_to_csv(1)\n",
    "    # obj.bayesian_modelling()\n",
    "    # # obj.posterior_ground_truth_distribution()\n",
    "    # obj.model_quality()\n",
    "    # obj.posterior_predictive_outcomes()\n",
    "    # journalist_value_counts.append(obj.plot_stacked_bar_plot())\n",
    "    # class_objects.append(obj)\n",
    "\n",
    "\n",
    "# journalists_same_path_history = []\n",
    "# print(len(journalist_value_counts))\n",
    "# for j in range(len(journalist_value_counts[0])):\n",
    "#     journalists_same_path_history.append(journalist_value_counts[0][j], journalist_value_counts[1][j], journalist_value_counts[2][j], journalist_value_counts[3][j])\n",
    "    \n",
    "all_journalists_same_path_history = []\n",
    "for i in range(len(journalist_value_counts[0])):\n",
    "    temp_arr = []\n",
    "    for j in range(len(journalist_value_counts)):\n",
    "        temp_arr.append(journalist_value_counts[j][i])\n",
    "    all_journalists_same_path_history.append(temp_arr)\n",
    "\n",
    "for i, path_history in enumerate(all_journalists_same_path_history):\n",
    "    label_map = {0 : \"Attacker (A)\", 1 : \"Bystander (B)\", 2 : \"Supporter (S)\", 3 : \"Journalist (J)\"}\n",
    "    mean_df = sum(path_history) / len(path_history)\n",
    "\n",
    "    variance_df = sum((df - mean_df) ** 2 for df in path_history) / len(path_history)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 3))\n",
    "    palette = sns.color_palette(\"pastel\", 4)\n",
    "    colors = [palette[3], palette[2], palette[0], palette[1]] \n",
    "    bottom = np.zeros(len(mean_df.columns))\n",
    "    new_axis_labels = []\n",
    "    for item in mean_df.columns:\n",
    "        latex_string = f\"${item[0]}_{str(int(item[-1])-1)} \\\\rightarrow {item[2]}_{item[-1]}$\"\n",
    "        new_axis_labels.append(latex_string)\n",
    "    for value in [0, 1, 2, 3]:\n",
    "        ax.bar(new_axis_labels, mean_df.loc[value], bottom=bottom, yerr=variance_df.loc[value], color=colors[value], label=f'{label_map[value]}', capsize=5)\n",
    "        bottom += mean_df.loc[value]\n",
    "\n",
    "    ax.set_xlabel(\"Path History ($GrandparentLabel_{GrandparentDepth} \\\\rightarrow ParentLabel_{ParentDepth}$)\")\n",
    "    ax.set_ylabel('Composition of Child label')\n",
    "    ax.set_title('Distribution of Child label given Grandparent label, Parent label, and Depth')\n",
    "\n",
    "\n",
    "    ax.legend(title='', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax = plt.gca()  # Get the current axis\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    # Set background color to light grey\n",
    "    ax.set_facecolor('#F0F0F0')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{i}.pdf', format='pdf', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
